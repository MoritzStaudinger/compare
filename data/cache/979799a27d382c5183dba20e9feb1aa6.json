{"summary": "# Scientific Fact-Checking: Retrieval Strategies and Datasets\n\nScientific fact-checking has emerged as a critical domain for combating misinformation, particularly as false information spreads rapidly through social media platforms. This overview examines the current landscape of scientific fact-checking, with a specific focus on retrieval strategies and datasets used in automated fact-checking systems.\n\n## Retrieval Strategies\n\nAutomated fact-checking systems typically employ a two-step retrieval process: document retrieval followed by rationale selection [1]. For document retrieval, systems identify relevant documents using named entities, noun phrases, and search APIs, while rationale selection involves extracting specific sentences from these documents as evidence [1].\n\nSeveral retrieval methods have proven effective in fact-checking systems. Traditional approaches like TF-IDF similarity serve as common baselines, while BM25 has demonstrated strong performance [1]. Some systems also leverage metadata, such as page viewership statistics, to enhance retrieval effectiveness [1].\n\nFact-checking approaches generally fall into two main categories: (1) verifying claims against trusted sources like Wikipedia, fact-checking websites, and academic papers, and (2) verifying claims against knowledge bases, though the latter faces coverage limitations [1]. The evidence used for verification can take multiple forms:\n\n1. **Unstructured evidence (text)** is most commonly used [2]\n2. **Structured knowledge** (knowledge bases and graphs) offers formalized information but suffers from incompleteness [2]\n3. **Semi-structured data** (tables) provides concise information presentation [2]\n4. **Combined evidence types** (text and tables) represent more recent approaches [2]\n\nA significant limitation of current retrieval approaches is that many systems assume evidence is already provided for claims, which isn't realistic for real-world applications [2]. Additionally, relying solely on Wikipedia restricts knowledge to a single source, while internet-retrieved evidence often lacks annotation and may include irrelevant information [2].\n\n## Datasets\n\nFact-checking datasets have evolved significantly in recent years. They are growing in size, increasingly capturing naturally occurring sentences, including more context and metadata, covering diverse domains, offering evidence chains, though many remain synthetic and imbalanced [1].\n\nSeveral major datasets and shared tasks have driven progress in the field:\n\n1. **FEVER and SCIVER** focus on evidence retrieval followed by validation [1]\n2. **UKP Snopes and RumourEval** retrieve relevant texts, determine stance, and then classify veracity [1]\n3. **VitaminC** was constructed based on factual revisions to Wikipedia [2]\n\nFor COVID-19 specifically, researchers have developed specialized datasets. The MM-COVID dataset is a multilingual and multidimensional fake news dataset containing 3,981 fake news items and 7,192 trustworthy items across six languages (English, Spanish, Portuguese, Hindi, French, and Italian), collected from social media platforms and fact-checking websites like Snopes and Poynter [3].\n\nThe SCORE program represents a large-scale effort to assess scientific credibility, using a Common Task Framework of approximately 30,000 articles from 2009-2018 across 10 disciplines, with 3,000 papers selected for additional annotation and 600 for reproduction or replication studies [4]. This initiative aims to develop scalable methods for assessing the credibility of claims in social-behavioral sciences.\n\n## Challenges and Limitations\n\nSeveral challenges persist in scientific fact-checking. Verdict classification has evolved from simple binary labels (true/false) to multi-class labels representing degrees of truthfulness, with recent datasets including varying numbers of labels (from 2 to 27) [2]. However, harmonizing veracity labels across sources remains challenging [2].\n\nThe effectiveness of fact-checking itself has been questioned, as debunking doesn't necessarily lead to information being corrected or removed, and cognitive biases like hyper-correction and backfire effects can cause people to forget corrections [5]. Fact-checkers may inadvertently spread the original false information during correction, and fact-checking may contribute to opinion polarization and echo chambers [5].\n\nRecent research has explored using large language models (LLMs) for misinformation detection. Experimental results with 7B parameter models (Llama2, Falcon, Mistral, and Orca) on a COVID-19 dataset showed that Mistral-7b significantly outperformed other models, achieving 83.04% accuracy in 5-shot settings [6]. This suggests promising directions for future fact-checking systems.\n\n## References\n\n[1] Abumansour, A. S., Zeng, X., & Zubiaga, A. (2022). Automated fact-checking: A survey. https://doi.org/10.1111/lnc3.12438\n\n[2] Guo, Z., Schlichtkrull, M., & Vlachos, A. (2022). A Survey on Automated Fact-Checking. https://doi.org/10.17863/cam.82097\n\n[3] Shushkevich, E. (2022). Fake News Detection in Online Platforms. https://doi.org/10.21427/mmbs-xv36\n\n[4] Alipourfard, N., Arendt, B., Benjamin, D. M., Benkler, Y., Bishop, M., Burstein, M., et al. (2021). Systematizing Confidence in Open Research and Evidence (SCORE). https://doi.org/10.31235/osf.io/46mnb\n\n[5] Giachanou, A., Rosso, P., Ruffo, G., & Semeraro, A. (2022). Studying fake news spreading, polarisation dynamics, and manipulation by bots: A tale of networks and language. https://doi.org/10.1016/j.cosrev.2022.100531\n\n[6] Hall, C. E., & Pendyala, V. S. (2024). Explaining Misinformation Detection Using Large Language Models. https://doi.org/10.3390/electronics13091673", "profiles": null, "input": {"prompt": "Can you give me an overview for the domain of scientific fact checking, especially focusing on retrieval strategies and datasets?", "model": "claude-3-7-sonnet-latest", "online": "true", "file_hash": null}, "feedback": [{"timestamp": "2025-06-03T13:34:04.984165", "rating": 4, "comments": "test"}]}